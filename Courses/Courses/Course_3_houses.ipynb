{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WALDO RIVIER 07.12.2018 Exercices module 3 Predicting houses prices\n",
    "---\n",
    "Applied Data Science : Machine Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "from scipy.linalg import lstsq\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import itertools\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class which implements utilities to access house price meta data \n",
    "--\n",
    "- columns, column's type, etc...\n",
    "- transformation of ordinal column code mapping, tansfomation of entire ordinal columns of a dataframe, etc...\n",
    "- meta data are stored in a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class meta_data:\n",
    "    _working_dir = None\n",
    "    _df_meta = None\n",
    "\n",
    "    def __init__(self, working_dir):\n",
    "        self._working_dir = working_dir\n",
    "        self._load_meta_data()\n",
    "\n",
    "    def _load_meta_data(self):\n",
    "        data_file = os.path.join(self._working_dir, 'meta_data.txt')\n",
    "\n",
    "        self._df_meta = pd.DataFrame.from_csv(data_file, sep='\\t')\n",
    "        self._df_meta.reset_index(inplace=True)\n",
    "        self._df_meta.ffill(inplace=True)\n",
    "\n",
    "    def _get_type_of_col(self, col):\n",
    "        try:\n",
    "            return self._df_meta[self._df_meta['column'] == col]['type'].iloc[0]\n",
    "        except:\n",
    "            print (\"Column not found in meta data \" + col)\n",
    "    \n",
    "    def get_cols_of_type(self, type):\n",
    "        cols = self._df_meta.copy()\n",
    "        cols = cols[cols.type==type]['column'].drop_duplicates()\n",
    "        return cols\n",
    "\n",
    "    def get_dict_ordinal(self, col):\n",
    "        dict_ordinal = None\n",
    "        if self._get_type_of_col(col) == 'Ordinal':\n",
    "            df_ordinal = self._df_meta[self._df_meta['column'] == col]\n",
    "            codes = df_ordinal.code.values\n",
    "            try:\n",
    "                codes = codes.astype(float)\n",
    "            except:\n",
    "                codes = [i.strip() for i in df_ordinal.code.values]\n",
    "         \n",
    "            dict_ordinal = dict(zip(codes, df_ordinal.ordinal_value.values))\n",
    "        return dict_ordinal\n",
    "\n",
    "    #------------------------------------------------------------------------------\n",
    "    # return dictionary build on code and code designation (used in special case\n",
    "    # for MS SubClass feature) \n",
    "    #------------------------------------------------------------------------------\n",
    "    def get_dict_nominal(self, col):\n",
    "        dict_nominal = None\n",
    "        if self._get_type_of_col(col) == 'Nominal':\n",
    "            df_nominal = self._df_meta[self._df_meta['column'] == col]\n",
    "            codes = df_nominal.code.values\n",
    "            try:\n",
    "                codes = codes.astype(int)\n",
    "            except:\n",
    "                codes = [i.strip() for i in df_nominal.code.values]\n",
    "         \n",
    "            dict_nominal = dict(zip(codes, df_nominal.code_designation.values))\n",
    "        return dict_nominal\n",
    "\n",
    "    def _map_col(self, df, col, type):\n",
    "        if df.columns.contains(col):\n",
    "            if self._get_type_of_col(col) == type:\n",
    "                dict_ordinal = self.get_dict_ordinal(col)\n",
    "                df[col] = df[col].map(dict_ordinal)\n",
    "    \n",
    "    def _map_ordinal_col(self, df, col):\n",
    "       self._map_col(df, col, 'Ordinal')\n",
    "       \n",
    "    # all ordinal columns's code will be associated with numerical values as defined in meta data\n",
    "    def map_ordinal_cols(self, df):\n",
    "        cols = self.get_cols_of_type('Ordinal')\n",
    "        for col in cols:\n",
    "            self._map_ordinal_col(df, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class which implements utilities to manage house price data\n",
    "--\n",
    "- encapsulate train data set\n",
    "- encapsulate data set to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sample_data:\n",
    "    _meta_data = None\n",
    "    _working_dir = None\n",
    "    _target = None\n",
    " \n",
    "    _df_test_data = None\n",
    "    _df_train_data = None\n",
    "    _df_train_data_orig = None\n",
    " \n",
    "    def __init__(self, working_dir, target, meta_data):\n",
    "        assert working_dir is not None and len(working_dir) > 0\n",
    "        assert target is not None and len(target) > 0\n",
    "        assert meta_data is not None\n",
    "        \n",
    "        self._working_dir = working_dir\n",
    "        self._meta_data = meta_data\n",
    "        self._target = target\n",
    "\n",
    "    def load_data(self):\n",
    "        self._load_train_data()\n",
    "        self._load_test_data()\n",
    "        \n",
    "        \n",
    "    def _load_train_data(self):\n",
    "        data_file = os.path.join(self._working_dir, 'house-prices.csv')\n",
    " \n",
    "        # save a copy of original train data     \n",
    "        self._df_train_data_orig = pd.read_csv(data_file)\n",
    " \n",
    "    #------------------------------------------------------------------------------ \n",
    "    # load the data set to predict Saleprice\n",
    "    #------------------------------------------------------------------------------ \n",
    "    def _load_test_data(self):\n",
    "        data_file = os.path.join(self._working_dir, 'house-prices-test.csv')\n",
    "        self._df_test_data = pd.read_csv(data_file)\n",
    "        \n",
    "    #------------------------------------------------------------------------------    \n",
    "    # Prepare data for the training phase\n",
    "    #------------------------------------------------------------------------------\n",
    "    def prepare_train_data(self):\n",
    "        assert self._df_train_data_orig is not None \n",
    "\n",
    "        # drop all columns which contains more than one NAN value\n",
    "        # motivations :\n",
    "        #   a. number of features removed is not dramatically important (20 on 81)\n",
    "        #   b. 7 features contains only 1 NAN value; without big effort and modification of \n",
    "        #        the informations, we can  easyly keep them\n",
    "\n",
    "        self._df_train_data = self._df_train_data_orig.dropna(thresh=2429, axis=1).copy()\n",
    "        self._df_train_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        # takes the log the target\n",
    "        self._df_train_data[self._target] = np.log(self._df_train_data[self._target])\n",
    "\n",
    "        # all ordinal features will be replaced with numerical (subjective) evaluations\n",
    "        self._meta_data.map_ordinal_cols(self._df_train_data)\n",
    "\n",
    "        # special case for features MS SubClass; apply transformation in order \n",
    "        # to use nominal text in place of numerical codes\n",
    "        col = 'MS SubClass'\n",
    "        dict_nominal = self._meta_data.get_dict_nominal(col)\n",
    "        self._df_train_data[col] = self._df_train_data[col].map(dict_nominal)\n",
    "\n",
    "        # apply some transformation to \"normalize\" the distribution\n",
    "        self.apply_transformation('Fireplaces')\n",
    "        self.apply_transformation('Year Built')\n",
    "        \n",
    "    #------------------------------------------------------------------------------\n",
    "    # return the prediction data; ordinal columns will be mapped\n",
    "    #------------------------------------------------------------------------------       \n",
    "    def prepare_prediction_data(self):\n",
    "        assert self._df_test_data is not None\n",
    "\n",
    "        self._meta_data.map_ordinal_cols(self._df_test_data) \n",
    "    \n",
    "    #------------------------------------------------------------------------------\n",
    "    # return the features which were removed in function prepare_train_data above\n",
    "    #------------------------------------------------------------------------------\n",
    "    def get_removed_cols(self): \n",
    "        return self._df_train_data_orig.columns.difference(set(self._df_train_data.columns))\n",
    "        \n",
    "    def get_cols_of_type(self, df, type):\n",
    "        cols = df.columns.intersection(self._meta_data.get_cols_of_type(type))\n",
    "        return cols\n",
    " \n",
    "    def get_cols_wo_target(self):\n",
    "        assert self._df_train_data is not None\n",
    "\n",
    "        cols = self._df_train_data.columns.copy()\n",
    "        cols = cols.drop(self._target)\n",
    "        cols = cols.drop(['Order', 'PID'])\n",
    "        return cols\n",
    "\n",
    "    #------------------------------------------------------------------------------\n",
    "    # iterate over all features (eventually encoded) and plot SalePrice as response\n",
    "    #------------------------------------------------------------------------------\n",
    "    def iterate_pair_plot(self):\n",
    "        col_cnt = len(self.get_cols_wo_target())\n",
    "\n",
    "        for i in np.arange(0, col_cnt):\n",
    "            self._pair_plot(i)\n",
    "   \n",
    "    def pair_plot(self, col):\n",
    "        assert self._df_train_data.columns.contains(col)\n",
    "\n",
    "        df = self._df_train_data.copy()\n",
    "        df = df[[self._target, col]]\n",
    "       \n",
    "        if self._meta_data._get_type_of_col(col) == 'Nominal':\n",
    "            df = pd.get_dummies(df, col)    \n",
    "\n",
    "        cols = df.columns.drop(self._target)\n",
    "        g = sns.pairplot(df, x_vars=cols,\n",
    "            y_vars=[self._target], \n",
    "            kind=\"reg\",\n",
    "            plot_kws={'line_kws':{'color':'red'}})\n",
    "        plt.show()\n",
    "\n",
    "    #------------------------------------------------------------------------------\n",
    "    # plot histogram of a features (eventually encoded) \n",
    "    #------------------------------------------------------------------------------\n",
    "    def hist_plot(self, col):\n",
    "        assert self._df_train_data.columns.contains(col)\n",
    "\n",
    "        df = self._df_train_data.copy()\n",
    "        df = df[col]\n",
    "      \n",
    "        if self._meta_data._get_type_of_col(col) == 'Nominal':\n",
    "            df = pd.get_dummies(df, col)    \n",
    "\n",
    "        df.hist()\n",
    "        plt.show()\n",
    "        \n",
    "    #------------------------------------------------------------------------------\n",
    "    # return the Saleprice distribution of train data set\n",
    "    #------------------------------------------------------------------------------\n",
    "    def get_train_distribution(self):\n",
    "        assert self._df_train_data is not None\n",
    "\n",
    "        return np.exp(self._df_train_data.SalePrice).describe()\n",
    "    \n",
    "    #------------------------------------------------------------------------------\n",
    "    # apply some transformations on some features\n",
    "    #------------------------------------------------------------------------------\n",
    "    def _indicator(self, col):\n",
    "        def _f_fire_places(x):\n",
    "            if x >= 1:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "            return _f\n",
    "\n",
    "        def _f_year_built(x):\n",
    "            if x > 1970:\n",
    "                return 'after_1970'\n",
    "            else:\n",
    "                return 'before_1970'\n",
    "\n",
    "        if col == 'Fireplaces':\n",
    "            return _f_fire_places\n",
    "        elif col == 'Year Built':\n",
    "            return _f_year_built\n",
    "        \n",
    "        \n",
    "    def apply_transformation(self, col):\n",
    "        assert self._df_train_data.columns.contains(col)\n",
    "\n",
    "        indicator = self._indicator(col)\n",
    "        assert indicator is not None\n",
    "        self._df_train_data[col] = self._df_train_data[col].apply(indicator)\n",
    "\n",
    "    #------------------------------------------------------------------------------\n",
    "    # inner class used to store result of runs\n",
    "    #------------------------------------------------------------------------------\n",
    "    class result:\n",
    "        _metric = None\n",
    "        comb_cols = None\n",
    "        cols = None\n",
    "        lr = None\n",
    "        train_score = None\n",
    "        test_score = None\n",
    "        test_baseline  = None\n",
    "        PID_pred = None\n",
    "        y_te = None\n",
    "        y_te_pred = None\n",
    "\n",
    "        def __init__(self, metric):\n",
    "            self._metric = metric\n",
    "\n",
    "        def as_dict(self):\n",
    "            return {'metric' : self._metric,\n",
    "                    'PID_test' : self.PID_pred,\n",
    "                    'comb_cols' : self.comb_cols,\n",
    "                    'cols' : self.cols,\n",
    "                    'lr' : self.lr,\n",
    "                    'train_score' : self.train_score,\n",
    "                    'test_score' : self.test_score,\n",
    "                    'test_baseline' : self.test_baseline,\n",
    "                    'y_te' : self.y_te,\n",
    "                    'y_te_pred' : self.y_te_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "return number of arrangments of k objects taken among n objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anp(n, k):\n",
    "    return math.factorial(n) / (math.factorial(k) * math.factorial(n - k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluates a model \n",
    "--\n",
    "\n",
    "- reg_type : type of regression to apply (linear, huber, ridge)\n",
    "- alpha : parameter for ridge regression\n",
    "- metric : metric to apply (median, mean)\n",
    "- sample_data : train data\n",
    "- comb_cols : sublist of features taken among train data \n",
    "- results : list of results into which the result of the run will be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(reg_type, alpha, metric, sample_data, comb_cols, results):\n",
    "    try:\n",
    "        df = sample_data._df_train_data.copy()\n",
    "        y = df[sample_data._target]\n",
    "        df = df[comb_cols]\n",
    "        \n",
    "        cols = sample_data.get_cols_of_type(df, 'Nominal')\n",
    "        if len(cols) > 0:\n",
    "            df = pd.get_dummies(df, columns=cols)\n",
    "\n",
    "        _X = df.values\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "           _X, y.values, train_size = 0.5, test_size = 0.5, random_state=1)\n",
    "\n",
    "        if reg_type == 'linear':\n",
    "            reg = LinearRegression()\n",
    "        elif reg_type == 'huber':\n",
    "            reg = HuberRegressor(1.35)\n",
    "            y_tr = y_tr.flatten()\n",
    "        elif reg_type == 'ridge':    \n",
    "            if alpha is not None: \n",
    "                reg = Ridge(alpha)\n",
    "\n",
    "        reg.fit(X_tr, y_tr)\n",
    "        y_pred_tr = reg.predict(X_tr)\n",
    "        y_pred_te = reg.predict(X_te)\n",
    "   \n",
    "        # stores the results\n",
    "        r = sample_data.result(metric)\n",
    "\n",
    "        # evaluate the metric\n",
    "        dummy = DummyRegressor(strategy=metric)\n",
    "        dummy.fit(X_tr, y_tr)\n",
    "        y_pred_base = dummy.predict(X_te)   \n",
    "\n",
    "        if metric == 'mean':\n",
    "            r.train_score = np.sqrt(mse(y_pred_tr, y_tr))\n",
    "            r.test_score = np.sqrt(mse(y_pred_te, y_te))\n",
    "            r.test_baseline = np.sqrt(mse(y_pred_base, y_te))\n",
    "        elif metric == 'median':\n",
    "            r.train_score = mae(y_pred_tr, y_tr)\n",
    "            r.test_score = mae(y_pred_te, y_te)\n",
    "            r.test_baseline = mae(y_pred_base, y_te)\n",
    "           \n",
    "        r.comb_cols = comb_cols\n",
    "        r.cols = df.columns\n",
    "        r.lr = reg\n",
    "        r.PID_test = None\n",
    "        r.y_te = y_te\n",
    "        r.y_te_pred = y_pred_te\n",
    "        results.append(r)\n",
    "        \n",
    "    except:\n",
    "        print(comb_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build prediction\n",
    "--\n",
    "\n",
    "- optimal train : best result of a list of runs\n",
    "- sample_data : data which contains the test data set to predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prediction(optimal_train, sample_data):\n",
    "    assert sample_data is not None\n",
    "    assert optimal_train is not None\n",
    "\n",
    "    try:\n",
    "        id = 'PID'\n",
    "        df = sample_data._df_test_data.copy()\n",
    "        PID = df[id]\n",
    "\n",
    "        df = df[optimal_train.comb_cols]\n",
    "        \n",
    "        cols = sample_data.get_cols_of_type(df, 'Nominal')\n",
    "        if len(cols) > 0:\n",
    "            df = pd.get_dummies(df, columns=cols)\n",
    "\n",
    "        # re-index test set in order to be compatible with train set \n",
    "        df = df.reindex(columns=optimal_train.cols)\n",
    "        df.fillna(0, inplace=True)\n",
    "\n",
    "        X = df.values\n",
    "        y_pred_te = optimal_train.lr.predict(X)\n",
    "  \n",
    "        prediction = sample_data.result(optimal_train.metric)\n",
    "        prediction.PID_test = PID\n",
    "        prediction.comb_cols = optimal_train.comb_cols\n",
    "        prediction.cols = df.columns\n",
    "\n",
    "        # transform \n",
    "        prediction.y_te_pred = np.exp(y_pred_te)\n",
    "\n",
    "        # format to a 2-column DataFrame\n",
    "        df = pd.concat([pd.DataFrame(prediction.PID_test), \n",
    "                        pd.DataFrame(prediction.y_te_pred)], axis=1)\n",
    "        df.columns = ['PID', 'SalePrice']\n",
    "        return df\n",
    "        \n",
    "    except:\n",
    "        print(optimal_train.comb_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class which implements functionalities to test and select an otptimal model\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_selector:\n",
    "    _prediction_base_file_name = 'house-prices-pred'\n",
    "    _sample_data = None\n",
    "    _train_results = None\n",
    "    _prediction = None\n",
    "\n",
    "    def __init__(self, sample_data):\n",
    "        assert sample_data is not None\n",
    "\n",
    "        self._sample_data = sample_data\n",
    " \n",
    "    def reset_run(self):\n",
    "        self._train_results = None\n",
    "        self._prediction = None\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # enumerates all possible combinations with k features\n",
    "    # perform and write perdictions\n",
    "    # limit : max combinations \n",
    "    # reg_type : linear, huber, ridge\n",
    "    #--------------------------------------------------------------------------\n",
    "    def run_combinations(self, reg_type, alpha, metric, k, limit):\n",
    "        self._sample_data.prepare_train_data()\n",
    "\n",
    "        cols = self._sample_data.get_cols_wo_target()\n",
    "        cols_cnt = len(cols)\n",
    "\n",
    "        if (cols_cnt >= k):\n",
    "            # avoid too many combinations to be evaluated\n",
    "            if (anp(cols_cnt, k) < 2000):\n",
    "                combinations = [list(x) for x in itertools.combinations(cols, k)]\n",
    "                combinations = [random.choice(combinations) for i in np.arange(limit)]\n",
    "\n",
    "                self._train_results = []\n",
    "                for combination in combinations:\n",
    "                    run_train(reg_type, alpha, metric, sample_data, combination, self._train_results)\n",
    "\n",
    "                self._sample_data.prepare_prediction_data()\n",
    "                self._prediction = build_prediction(self._find_optimal_train(), self._sample_data)\n",
    "  \n",
    "    #--------------------------------------------------------------------------\n",
    "    # add features to an (eventually already evaluated) optimal combination \n",
    "    # in order to build a more accurate model.\n",
    "    # a new optimal train will be evaluated\n",
    "    #--------------------------------------------------------------------------\n",
    "    def run_combination(self, reg_type, alpha, metric, cols):\n",
    "        self._sample_data.prepare_train_data()\n",
    "\n",
    "        # when no optimal train already exists, \n",
    "        try :\n",
    "            opt_train = self._find_optimal_train()\n",
    "            combination = opt_train.comb_cols.copy()\n",
    "        except:\n",
    "            combination = []\n",
    "        combination.extend(cols)\n",
    "\n",
    "        self._train_results = []\n",
    "        run_train(reg_type, alpha, metric, sample_data, combination, self._train_results)\n",
    "\n",
    "        sample_data.prepare_prediction_data()\n",
    "        self._prediction = build_prediction(self._find_optimal_train(), sample_data)\n",
    "   \n",
    "    #--------------------------------------------------------------------------\n",
    "    # run grid seach ridge regression with given features \n",
    "    #--------------------------------------------------------------------------\n",
    "    def run_ridge_grid(self, metric, cols):\n",
    "        self._sample_data.prepare_train_data()\n",
    "        \n",
    "        self._train_results = []\n",
    "        for alpha in np.logspace(-2, 10, num=100):\n",
    "            run_train('ridge', alpha, metric, sample_data, cols, self._train_results)\n",
    "        \n",
    "        # plot ridge grid results\n",
    "        if self._train_results is not None:\n",
    "            df = pd.DataFrame([x.as_dict() for x in self._train_results])\n",
    "            alphas = [x.alpha for x in df.lr]\n",
    "\n",
    "            plt.semilogx(alphas, df.train_score, label='mse train curve')\n",
    "            plt.semilogx(alphas, df.test_score, label='mse test curve')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            sample_data.prepare_prediction_data()\n",
    "            self._prediction = build_prediction(self._find_optimal_train(), sample_data)\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # find an optimal test score among all train's run\n",
    "    #--------------------------------------------------------------------------\n",
    "    def _find_optimal_train(self):\n",
    "        assert self._train_results is not None\n",
    "\n",
    "        df = pd.DataFrame([x.as_dict() for x in self._train_results])\n",
    "        i_opt = df['test_score'].idxmin()\n",
    "        return df.iloc[i_opt,:]\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # get Saleprice distibution of the optimal train \n",
    "    #--------------------------------------------------------------------------\n",
    "    def get_prediction_distribution(self):\n",
    "        assert self._prediction is not None\n",
    "\n",
    "        return self._prediction.SalePrice.describe()\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # write prediction data to a csv file\n",
    "    #--------------------------------------------------------------------------\n",
    "    def _write_prediction(self, suffix):\n",
    "        assert self._prediction is not None\n",
    "   \n",
    "        if suffix is not None:\n",
    "            file_name = self._prediction_base_file_name  + str('-') + (suffix)\n",
    "        file_name += str('.csv')\n",
    "\n",
    "        data_file = os.path.join(self._sample_data._working_dir, file_name)\n",
    "        self._prediction.to_csv(data_file, index=False)\n",
    "  \n",
    "    #--------------------------------------------------------------------------\n",
    "    # plot optimal train\n",
    "    #--------------------------------------------------------------------------\n",
    "    def _plot_optimal_train(self):\n",
    "        opt = self._find_optimal_train()\n",
    "\n",
    "        values = [opt.test_baseline,  \n",
    "                  opt.train_score,\n",
    "                  opt.test_score, ]\n",
    "  \n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        bar_width = 0.6\n",
    "        index = np.arange(len(values))\n",
    "        rects1 = ax.bar(index, values, bar_width, color='g')\n",
    "        ax.set_xlabel('metrics')\n",
    "        ax.set_ylabel('mse score')\n",
    "        ax.set_title(opt.comb_cols)\n",
    "        ax.set_xticks(index)\n",
    "\n",
    "        ax.set_xticklabels(('baseline test score', 'train score', 'test score'))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluations\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\waldo\\source\\repos\\study_1\\courses\\courses\\env\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'C:\\\\Users\\\\Waldo\\\\source\\\\repos\\\\study_1\\\\Courses\\\\Courses\\\\course_projects\\\\course_projects\\\\data\\\\module_3\\\\meta_data.txt' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ef34da3065ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mworking_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworking_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'course_projects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'module_3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmeta_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworking_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0msample_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworking_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SalePrice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0msample_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-3efe39357b78>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, working_dir)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworking_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_working_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mworking_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_meta_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_load_meta_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-3efe39357b78>\u001b[0m in \u001b[0;36m_load_meta_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mdata_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_working_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'meta_data.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df_meta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df_meta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df_meta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mffill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\waldo\\source\\repos\\study_1\\courses\\courses\\env\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mfrom_csv\u001b[1;34m(cls, path, header, sep, index_col, parse_dates, encoding, tupleize_cols, infer_datetime_format)\u001b[0m\n\u001b[0;32m   1577\u001b[0m                           \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1578\u001b[0m                           \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtupleize_cols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtupleize_cols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1579\u001b[1;33m                           infer_datetime_format=infer_datetime_format)\n\u001b[0m\u001b[0;32m   1580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1581\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'block'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\waldo\\source\\repos\\study_1\\courses\\courses\\env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\waldo\\source\\repos\\study_1\\courses\\courses\\env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\waldo\\source\\repos\\study_1\\courses\\courses\\env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\waldo\\source\\repos\\study_1\\courses\\courses\\env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\waldo\\source\\repos\\study_1\\courses\\courses\\env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'C:\\\\Users\\\\Waldo\\\\source\\\\repos\\\\study_1\\\\Courses\\\\Courses\\\\course_projects\\\\course_projects\\\\data\\\\module_3\\\\meta_data.txt' does not exist"
     ]
    }
   ],
   "source": [
    "working_dir = os.getcwd()\n",
    "working_dir = os.path.join(working_dir,'course_projects', 'data', 'module_3')\n",
    "\n",
    "meta_data = meta_data(working_dir)\n",
    "sample_data = sample_data(working_dir, 'SalePrice', meta_data)\n",
    "sample_data.load_data()\n",
    "\n",
    "# dictionnary to store the results\n",
    "model_optimal_train_results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
